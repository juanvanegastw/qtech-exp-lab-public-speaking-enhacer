# Public Speaking Tech / Protos

## RHEMA  
A real time system to give people feedback on public speaking
[Video](https://www.youtube.com/watch?time_continue=2&v=9Ml0cOV_CjA&feature=emb_logo)
[Article](https://phys.org/news/2015-03-wearable-technology.html)

## Wearable Doppel  
Doppel conducted a study to prove its tactile heartbeat tech calms you down
[Article](https://www.wareable.com/wearable-tech/doppel-study-tactile-heartbeat-public-speaking-8887)

## Spire Breath analysis  
Can this tracker bring you us inner calm or is it just another thing to stress about? 
[Article](https://www.wareable.com/wearable-tech/spire-review)

## Real Time audience measurement  
### Quantified Crowd  
During each talk, the audience unique emotional response and engagement (attention level) was sensed and tracked in real-time. 
[Article](https://sightcorp.com/portfolio/tedxamsterdam/)
### Crowdemotion
Audience analysis of mp4 vdeo:
* [Project](https://memo.crowdemotion.co.uk/)
* [API](https://developers.crowdemotion.co.uk/api/)


## 6 public speaking apps
[Link](https://www.scienceofpeople.com/public-speaking-apps/)

## Librosa
Speech  Emotion Recognizers Projects 
using Python, Scikit, Librosa, Neural Networks:
* [First Tutorial](https://www.thepythoncode.com/article/building-a-speech-emotion-recognizer-using-sklearn)
* [Second Tutorial](https://data-flair.training/blogs/python-mini-project-speech-emotion-recognition/)
* [Article](https://mc.ai/speech-emotion-detection/)
* [Code Source](https://librosa.github.io/librosa/)

Properties it recognizes:
* mfcc: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound
* chroma: Pertains to the 12 different pitch classes
* mel: Mel Spectrogram Frequency
## Training Data Sets at RAVDESS:
[Kaggle Project](https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio#03-01-01-01-01-01-02.wav)
Typical Process:
* Preparing the Dataset: Here, we download and convert the dataset to be suited for extraction.
* Loading the Dataset: This process is about loading the dataset in Python which involves extracting audio features, such as obtaining different features such as power, pitch and vocal tract configuration from the speech signal, we will use librosa library to do that.  Training the Model: After we prepare and load the dataset, we simply train it on a suited sklearn model.
* Testing the Model: Measuring how good our model is doing.

--- 
AI (old) Microsoft repository (various platforms and applications)
https://github.com/microsoft/ProjectOxford-ClientSDK

